"""
XGBoost Regressor Pipeline
Author: Palak Porwal
Last Updated: April 2025

This module builds a comprehensive XGBoost regression pipeline using:
- Domain-specific feature engineering
- Preprocessing with scaling, feature selection, and dimensionality reduction
- Polynomial features for non-linear interactions
- Grid search for parameter tuning
"""

import pandas as pd
import numpy as np
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from xgboost import XGBRegressor
from sklearn.model_selection import GridSearchCV, cross_val_score
from sklearn.feature_selection import SelectKBest, f_regression
from sklearn.impute import SimpleImputer
from sklearn.decomposition import PCA
from sklearn.compose import ColumnTransformer
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
import logging
import warnings

warnings.filterwarnings("ignore")
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class SupplyChainFeatureGenerator(BaseEstimator, TransformerMixin):
    """
    Custom transformer to derive supply chain-specific features
    such as buffer ratios and volatility index.
    """
    def __init__(self):
        pass

    def fit(self, X, y=None):
        return self

    def transform(self, X):
        X_ = X.copy()

        if 'inventory_days' in X_.columns and 'lead_time' in X_.columns:
            X_['buffer_ratio'] = X_['inventory_days'] / (X_['lead_time'] + 1)

        if 'delivery_variability' in X_.columns and 'supplier_reliability' in X_.columns:
            X_['reliability_index'] = (1 - X_['delivery_variability']) * X_['supplier_reliability']

        return X_


def build_xgboost_model(X_train, y_train):
    logger.info("▶ Initializing XGBoost pipeline...")

    numeric_features = X_train.select_dtypes(include=['float64', 'int64']).columns.tolist()

    # Preprocessing pipeline
    numeric_pipeline = Pipeline(steps=[
        ('impute', SimpleImputer(strategy='mean')),
        ('scale', StandardScaler()),
        ('select', SelectKBest(score_func=f_regression, k='all')),
        ('poly', PolynomialFeatures(degree=2, include_bias=False)),
        ('reduce', PCA(n_components=0.95, random_state=42))
    ])

    preprocessor = ColumnTransformer(transformers=[
        ('numeric', numeric_pipeline, numeric_features)
    ])

    # Full modeling pipeline
    full_pipeline = Pipeline(steps=[
        ('features', SupplyChainFeatureGenerator()),
        ('prep', preprocessor),
        ('model', XGBRegressor(objective='reg:squarederror', random_state=42, verbosity=0))
    ])

    # Hyperparameter space
    param_grid = {
        'model__n_estimators': [100, 200, 300],
        'model__max_depth': [3, 5, 7],
        'model__learning_rate': [0.01, 0.05, 0.1],
        'model__subsample': [0.8, 1.0],
        'model__colsample_bytree': [0.8, 1.0],
        'model__reg_alpha': [0, 0.1],
        'model__reg_lambda': [1, 1.5]
    }

    logger.info("▶ Running GridSearchCV for XGBoost Regressor...")

    grid_search = GridSearchCV(
        estimator=full_pipeline,
        param_grid=param_grid,
        scoring='neg_root_mean_squared_error',
        cv=5,
        verbose=2,
        n_jobs=-1,
        return_train_score=True
    )

    grid_search.fit(X_train, y_train)

    logger.info("✅ Best XGBoost Parameters Found:")
    for param, val in grid_search.best_params_.items():
        logger.info(f"   • {param}: {val}")

    logger.info("✅ Best RMSE: %.4f", -grid_search.best_score_)

    best_model = grid_search.best_estimator_

    # Cross-validation to report detailed RMSE variance
    logger.info("▶ Performing cross-validation for RMSE stability check...")
    cv_scores = -cross_val_score(best_model, X_train, y_train, cv=5, scoring='neg_root_mean_squared_error')
    logger.info("✅ CV RMSE Scores: %s", np.round(cv_scores, 4))
    logger.info("✅ Mean RMSE: %.4f", np.mean(cv_scores))

    return best_model

"""
Gradient Boosting Regressor Pipeline
Author: Palak Porwal
Last Updated: April 2025

This module defines a full-featured pipeline for regression using
Gradient Boosting. It includes:
- Domain-aware feature engineering
- Preprocessing with scaling, feature selection, and PCA
- Polynomial interactions
- Exhaustive grid search
"""

import pandas as pd
import numpy as np
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.model_selection import GridSearchCV, cross_val_score
from sklearn.feature_selection import SelectKBest, f_regression
from sklearn.impute import SimpleImputer
from sklearn.decomposition import PCA
from sklearn.compose import ColumnTransformer
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
import logging

# Setup logging for diagnostics and performance tracing
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class DomainFeatureAdder(BaseEstimator, TransformerMixin):
    """
    Custom transformer to add supply chain-specific features like
    inventory-to-lead-time ratio, risk-normalized demand score, etc.
    """
    def __init__(self):
        pass

    def fit(self, X, y=None):
        return self

    def transform(self, X):
        X = X.copy()
        if 'inventory_days' in X.columns and 'lead_time' in X.columns:
            X['inventory_efficiency'] = X['inventory_days'] / (X['lead_time'] + 1)

        if 'demand_volatility' in X.columns and 'risk_index' in X.columns:
            X['volatility_risk_ratio'] = X['demand_volatility'] / (X['risk_index'] + 1)

        return X


def build_gradient_boosting_model(X_train, y_train):
    logger.info("▶ Initiating Gradient Boosting Regressor pipeline...")

    numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()

    # Preprocessing pipeline
    numeric_pipeline = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='mean')),
        ('scaler', StandardScaler()),
        ('select', SelectKBest(score_func=f_regression, k='all')),
        ('poly', PolynomialFeatures(degree=2, include_bias=False)),
        ('pca', PCA(n_components=0.95, random_state=42))
    ])

    preprocessor = ColumnTransformer(transformers=[
        ('num', numeric_pipeline, numeric_features)
    ])

    # Full pipeline with custom feature addition
    full_pipeline = Pipeline(steps=[
        ('feature_addition', DomainFeatureAdder()),
        ('preprocessing', preprocessor),
        ('regressor', GradientBoostingRegressor(random_state=42))
    ])

    # Hyperparameter search space
    param_grid = {
        'regressor__n_estimators': [100, 200, 300],
        'regressor__learning_rate': [0.01, 0.05, 0.1],
        'regressor__max_depth': [3, 5, 7],
        'regressor__subsample': [0.8, 1.0],
        'regressor__min_samples_split': [2, 5],
        'regressor__loss': ['squared_error', 'huber']
    }

    logger.info("▶ Starting GridSearchCV for Gradient Boosting...")

    grid_search = GridSearchCV(
        estimator=full_pipeline,
        param_grid=param_grid,
        cv=5,
        verbose=2,
        n_jobs=-1,
        scoring='neg_root_mean_squared_error',
        return_train_score=True
    )

    grid_search.fit(X_train, y_train)

    logger.info("✅ Best Gradient Boosting Parameters: %s", grid_search.best_params_)
    logger.info("✅ Best Cross-Validation RMSE: %.4f", -grid_search.best_score_)

    best_model = grid_search.best_estimator_

    logger.info("▶ Running post-tuning cross-validation...")
    cv_scores = -cross_val_score(best_model, X_train, y_train, cv=5, scoring='neg_root_mean_squared_error')
    logger.info("✅ Cross-validated RMSE scores: %s", np.round(cv_scores, 4))
    logger.info("✅ Mean RMSE: %.4f", np.mean(cv_scores))

    return best_model
